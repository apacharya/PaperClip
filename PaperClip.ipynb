{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing PaperClip: A Personal Learning Assistant that learns and answers questions about scientific paper of interest from arxiv, in a readable, short and digestable format. \n",
    "I am building it using LangChain and OpenAI model (gpt-3.5-turbo)\n",
    "\n",
    "#### Inputs: \n",
    "- Input a arxiv paper ID as input (ex: 1706.03762)\n",
    "- user question (input)\n",
    "\n",
    "\n",
    "##### Output:\n",
    "smart answer - a simple summary of scientific paper of interest\n",
    "\n",
    "##### Concepts I am demostrating are below:\n",
    "- Role prompting to mimic learning assistant role \n",
    "- Vector database to store the data source and support semantic search to retrieve relevant context\n",
    "- Personalized response \n",
    "\n",
    "\n",
    "\n",
    "#### Evaluation:\n",
    "\n",
    "\n",
    "\n",
    "#### References/Credit:\n",
    "https://arxiv.org/pdf/2309.15217.pdf\n",
    "https://medium.com/@akash.hiremath25/unlocking-the-power-of-intelligence-building-an-application-with-gemini-python-and-faiss-for-eb9a055d2429\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# update or install the necessary libraries\n",
    "!pip install --upgrade openai\n",
    "!pip install -U sentence-transformers\n",
    "!pip install loader\n",
    "!pip install lxml\n",
    "!pip install arxiv\n",
    "!pip install PyPDF2\n",
    "!pip install tiktoken\n",
    "!pip install langchain\n",
    "!pip install sentence-transformers\n",
    "!pip install numpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain.llms import OpenAI\n",
    "import IPython\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# API configuration\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.document_loaders.parsers import BS4HTMLParser, PDFMinerParser\n",
    "from langchain.document_loaders.parsers.generic import MimeTypeBasedParser\n",
    "from langchain.document_loaders.parsers.txt import TextParser\n",
    "from langchain_community.document_loaders import Blob\n",
    "import requests\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "import arxiv\n",
    "import PyPDF2\n",
    "import io\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain import hub\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Function to convert PDF to text\n",
    "def pdf_to_txt(input_file):\n",
    "    with open(input_file, 'rb') as f:\n",
    "        pdf_reader = PyPDF2.PdfReader(f)\n",
    "        text = ''\n",
    "\n",
    "        # Extract text from each page of the PDF\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            text += pdf_reader.pages[page_num].extract_text()\n",
    "    \n",
    "    return text\n",
    "\n",
    "#  Function to prepare the index for querying\n",
    "# \n",
    "def prepare_index(output_file):\n",
    "     loader = TextLoader(output_file)\n",
    "     index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "     return index\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are step by step process I followed to build PaperClip\n",
    "\n",
    "- Step 1: User is askied to provide Arxiv ID of scientific paper of his/her interest. I am using arxiv to the download that paper.\n",
    "- Step 2: Convert the paper pdf version into text for better loading and parsing\n",
    "- Step 3: Creating CharacterTextSplitter to split the paper content into chunks of size 3000 tokens with overlap of 100 between chunks\n",
    "- Step 4: Creating embeddings of the chunks and storing them in vector databse (ChromaDB)\n",
    "- Step 5: Using similarity search (cosine similarity) to retrive relevant chunks of context\n",
    "- Step 6: Using langchain's load_qa_with_sources_chain to generate response based on the retrived information. Returning output answer only. Here I'm using several prompt engineering techniques to retrive answer differently and refine it to be more closer to what user might expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0m/d5qp0g8x6l18qjc_gydmnq9r0000gp/T/ipykernel_79212/547467742.py:7: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  paper = next(search.results())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#input_file = '/Users/apoorvaacharya/Documents/GitHub/maven-pe-for-llms-8/PaperClip/PaperLLM/PaperDB/2303.18223.pdf'\n",
    "#output_file = '/Users/apoorvaacharya/Documents/GitHub/maven-pe-for-llms-8/PaperClip/PaperLLM/PaperDB/2303.18223.txt'\n",
    "\n",
    "##Ask for Arxiv ID and prepare the index\n",
    "arxiv_id = input(\"Enter Arxiv ID (e.g. 2303.17580): \")\n",
    "search = arxiv.Search(id_list=[arxiv_id])\n",
    "paper = next(search.results())\n",
    "paper.download_pdf(filename=\"downloaded-paper.pdf\")\n",
    "\n",
    "input_file = 'downloaded-paper.pdf'\n",
    "output_file = 'converted-paper.txt'\n",
    "\n",
    "## loading data\n",
    "text = pdf_to_txt(input_file)\n",
    "\n",
    "# Save the text to a file\n",
    "with io.open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(text)\n",
    "\n",
    "# Prepare the index for querying\n",
    "index = prepare_index(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEthod 1: Splitting the text\n",
    "\n",
    "#text_splitter = CharacterTextSplitter(chunk_size=3000, chunk_overlap=100, separator=\"\\n\")\n",
    "#text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=100, separator=\"\\n\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap =  50)\n",
    "texts = text_splitter.split_text(text)\n",
    "\n",
    "# embeddings  from OpenAI\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "## creating vector store\n",
    "docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))])\n",
    "\n",
    "## rag retrieval based on similarity search\n",
    "query = \"What is the paper title?\"\n",
    "docs = docsearch.similarity_search_with_relevance_scores(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='- August 4, Volume 1: Long Papers , 2017, pp. 1601–1611.\\n[559] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi,\\n“PIQA: reasoning about physical commonsense in\\nnatural language,” in The Thirty-Fourth AAAI Confer-\\nence on Artificial Intelligence, AAAI 2020, The Thirty-\\nSecond Innovative Applications of Artificial Intelligence\\nConference, IAAI 2020, The Tenth AAAI Symposium\\non Educational Advances in Artificial Intelligence, EAAI\\n2020, New York, NY, USA, February 7-12, 2020 , 2020,', metadata={'source': '1424'}),\n",
       " Document(page_content='tational Linguistics (Volume 1: Long Papers), ACL 2022,\\nDublin, Ireland, May 22-27, 2022 , S. Muresan, P . Nakov,\\nand A. Villavicencio, Eds., 2022, pp. 3470–3487.\\n[167] S. H. Bach, V . Sanh, Z. X. Yong, A. Webson, C. Raffel,\\nN. V . Nayak, A. Sharma, T. Kim, M. S. Bari, T. F ´evry,\\nZ. Alyafeai, M. Dey, A. Santilli, Z. Sun, S. Ben-David,\\nC. Xu, G. Chhablani, H. Wang, J. A. Fries, M. S.\\nAlShaibani, S. Sharma, U. Thakker, K. Almubarak,\\nX. Tang, D. R. Radev, M. T. Jiang, and A. M. Rush,', metadata={'source': '1219'}),\n",
       " Document(page_content='A. Moi, P . Cistac, T. Rault, R. Louf, M. Funtowicz,\\nJ. Davison, S. Shleifer, P . von Platen, C. Ma, Y. Jer-\\nnite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame,\\nQ. Lhoest, and A. M. Rush, “Transformers: State-of-\\nthe-art natural language processing,” in Proceedings of\\nthe 2020 Conference on Empirical Methods in Natural Lan-\\nguage Processing: System Demonstrations, EMNLP 2020\\n- Demos, Online, November 16-20, 2020 . Association\\nfor Computational Linguistics, 2020, pp. 38–45.', metadata={'source': '1231'}),\n",
       " Document(page_content='ation for Computational Linguistics, 2018, pp. 2369–\\n2380.\\n[580] C. Clark, K. Lee, M. Chang, T. Kwiatkowski,\\nM. Collins, and K. Toutanova, “Boolq: Exploring the\\nsurprising difficulty of natural yes/no questions,” in\\nProceedings of the 2019 Conference of the North American\\nChapter of the Association for Computational Linguistics:\\nHuman Language Technologies, NAACL-HLT 2019, Min-\\nneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and\\nShort Papers) , J. Burstein, C. Doran, and T. Solorio, Eds.', metadata={'source': '1438'})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# contexts = list(map(lambda doci: doci[0].page_content, docs))\n",
    "# scores =  list(map(lambda doci: doci[1], docs))\n",
    "\n",
    "# context_dict = {\"contexts\": contexts,\"scores\": scores}\n",
    "# #context_dict = sorted(context_dict)\n",
    "# context_dict\n",
    "\n",
    "## Contexts retrieved are below \n",
    "docs_iter = [item[0] for item in docs]\n",
    "docs_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#cc = list(map(lambda doci: doci[0].page_content, docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Langchain Q&A chain to generate the responses using OpenAI model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Step 1. Answer generation without prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': ' The paper title is \"Transformers: State-of-the-art natural language processing.\"\\nSOURCES: 1231'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## modelling response generation without prompting\n",
    "\n",
    "chain = load_qa_with_sources_chain(OpenAI(temperature=0.3), chain_type=\"stuff\")\n",
    "chain({\"input_documents\": docs_iter, \"question\": query}, return_only_outputs=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Main loop for queries\n",
    "# while True:\n",
    "#     # Ask for the user's query\n",
    "#     user_query = input(\"Enter your query (or type 'exit' to quit): \")\n",
    "#     print(user_query)\n",
    "#     if user_query.lower() == 'exit':\n",
    "#         break\n",
    "\n",
    "#     # Query the index if it exists\n",
    "#     if index:\n",
    "#         response = index.query(user_query)\n",
    "#         print(f\"Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Answer with Prompting\n",
    "##### Step 2. Modelling response generation with prompting to return only based on the context retrived "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PIQA: reasoning about physical commonsense in natural language'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## modelling response generation with prompting to return only based on the context retrived \n",
    "\n",
    "retriever = docsearch.as_retriever()\n",
    "#prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "template = \"\"\"You are a personal learning agent. \n",
    "Output \"NA\" if you are not able to answer the question. \n",
    "Answer the question based only on the following context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OpenAI(temperature=0.3)\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "answer2 = chain.invoke(query)\n",
    "answer2\n",
    "\n",
    "#answer_withprompt = output['output_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3. Response generation with prompting  not limited to the context retrived\n",
    "learning - Its not reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': '\\nAnswer: \"Transformers: State-of-the-art natural language processing\" (SOURCES: 1231)'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer. Keep the answer short.\n",
    "ALWAYS return a \"SOURCES\" part in your answer. \n",
    "\n",
    "=========\n",
    "{summaries}\n",
    "=========\n",
    "\n",
    "Given the summary above, help answer the following question from the user:\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt template\n",
    "PROMPT = PromptTemplate(template=template, input_variables=[\"summaries\", \"question\"])\n",
    "\n",
    "# query \n",
    "chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\", prompt=PROMPT)\n",
    "chain({\"input_documents\": docs_iter, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer without prompt to translate into different language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIl titolo del paper è \"Transformers: State-of-the-art natural language processing\".'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### language translation prompt\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in the mentioned language: {language}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"language\": itemgetter(\"language\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": query, \"context\": itemgetter(\"question\") | retriever, \"language\": \"italian\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_arrow_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m eval_arrow_dataset\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_arrow_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "eval_arrow_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other method\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_text(text)\n",
    "\n",
    "# embeddings  from OpenAI\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "vectorstore = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))])\n",
    "\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = docsearch.as_retriever()\n",
    "#prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=1)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | texts, \"question\": RunnablePassthrough()}\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_community.document_transformers import DoctranQATransformer\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "documents = [Document(page_content=text)]\n",
    "qa_transformer = DoctranQATransformer()\n",
    "transformed_document = qa_transformer.transform_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(messages, model=\"gpt-3.5-turbo-0613\", temperature=0, max_tokens=300):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pe-for-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
