,documents,contexts,answer,question
0,2303.18223.pdf,"[['sociation for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2018, New Orleans, Louisiana,\nUSA, June 1-6, 2018, Volume 1 (Long Papers) , M. A.\nWalker, H. Ji, and A. Stent, Eds. Association for\nComputational Linguistics, 2018, pp. 2227–2237.\n[22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin,\n“Attention is all you need,” in Advances in Neural\nInformation Processing Systems 30: Annual Conference on\nNeural Information Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA , 2017, pp. 5998–6008.\n[23] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT:\npre-training of deep bidirectional transformers for\nlanguage understanding,” in Proceedings of the 2019\nConference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers) ,']
 ['Human Language Technologies, NAACL-HLT 2019, Min-\nneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and\nShort Papers) , J. Burstein, C. Doran, and T. Solorio, Eds.\nAssociation for Computational Linguistics, 2019, pp.\n2924–2936.\n[581] M. Sap, H. Rashkin, D. Chen, R. L. Bras, and Y. Choi,\n“Socialiqa: Commonsense reasoning about social in-\nteractions,” CoRR , vol. abs/1904.09728, 2019.\n[582] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and\nY. Choi, “Hellaswag: Can a machine really finish\nyour sentence?” in Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Volume 1: Long\nPapers , A. Korhonen, D. R. Traum, and L. M `arquez,\nEds. Association for Computational Linguistics, 2019,\npp. 4791–4800.\n[583] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi,\n“Winogrande: An adversarial winograd schema chal-\nlenge at scale,” in AAAI . AAAI Press, 2020, pp. 8732–\n8740.']
 ['Computational Linguistics, 2020, pp. 1–55.\n[544] F. Akhbardeh, A. Arkhangorodsky, M. Biesialska,\nO. Bojar, R. Chatterjee, V . Chaudhary, M. R. Costa-\njuss`a, C. Espa ˜na-Bonet, A. Fan, C. Federmann, M. Fre-\nitag, Y. Graham, R. Grundkiewicz, B. Haddow, L. Har-\nter, K. Heafield, C. Homan, M. Huck, K. Amponsah-\nKaakyire, J. Kasai, D. Khashabi, K. Knight, T. Kocmi,\nP . Koehn, N. Lourie, C. Monz, M. Morishita, M. Na-\ngata, A. Nagesh, T. Nakazawa, M. Negri, S. Pal,\nA. A. Tapo, M. Turchi, V . Vydrin, and M. Zampieri,\n“Findings of the 2021 conference on machine transla-\ntion (WMT21),” in Proceedings of the Sixth Conference\non Machine Translation, WMT@EMNLP 2021, Online\nEvent, November 10-11, 2021 , L. Barrault, O. Bojar,\nF. Bougares, R. Chatterjee, M. R. Costa-juss `a, C. Fe-\ndermann, M. Fishel, A. Fraser, M. Freitag, Y. Graham,\nR. Grundkiewicz, P . Guzman, B. Haddow, M. Huck,\nA. Jimeno-Yepes, P . Koehn, T. Kocmi, A. Martins,\nM. Morishita, and C. Monz, Eds. Association for']
 ['Annual Conference of the International Speech Commu-\nnication Association, Florence, Italy, August 27-31, 2011 .\nISCA, 2011, pp. 2877–2880.\n[19] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean, “Distributed representations of words and\nphrases and their compositionality,” in Advances in\nNeural Information Processing Systems 26: 27th Annual\nConference on Neural Information Processing Systems\n2013. Proceedings of a meeting held December 5-8, 2013,\nLake Tahoe, Nevada, United States , C. J. C. Burges, L. Bot-\ntou, Z. Ghahramani, and K. Q. Weinberger, Eds., 2013,\npp. 3111–3119.\n[20] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Ef-\nficient estimation of word representations in vector\nspace,” in 1st International Conference on Learning Rep-\nresentations, ICLR 2013, Scottsdale, Arizona, USA, May\n2-4, 2013, Workshop Track Proceedings , Y. Bengio andY. LeCun, Eds., 2013.\n[21] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner,\nC. Clark, K. Lee, and L. Zettlemoyer, “Deep contex-']]","
Attention is all you need",What is the title of this paper?
1,2403.08770.pdf,"[['problem: recent algorithmic advances . Springer Nature,\n2022. 2\n[16] Tat-Jun Chin, Zhipeng Cai, and Frank Neumann. Robust\nfitting in computer vision: Easy or hard? In Proceedings\nof the European Conference on Computer Vision (ECCV) ,\npages 701–716, 2018. 2\n[17] Christopher Choy, Jaesik Park, and Vladlen Koltun. Fully\nconvolutional geometric features. In Proceedings of the\nIEEE/CVF international conference on computer vision ,\npages 8958–8966, 2019. 7, 12, 13\n[18] Christopher Choy, Wei Dong, and Vladlen Koltun. Deep\nglobal registration. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition , pages\n2514–2523, 2020. 6, 7, 13, 14\n[19] Ondrej Chum and Jiri Matas. Matching with prosac-\nprogressive sample consensus. In 2005 IEEE computer so-\nciety conference on computer vision and pattern recognition\n(CVPR’05) , pages 220–226. IEEE, 2005. 2\n[20] Brian Curless and Marc Levoy. A volumetric method for\nbuilding complex models from range images. In Proceedings']
 ['sion—ECCV 2002: 7th European Conference on Computer\nVision Copenhagen, Denmark, May 28–31, 2002 Proceed-\nings, Part I 7 , pages 82–96. Springer, 2002. 2\n[62] Philip H. S. Torr. Bayesian model estimation and selection\nfor epipolar geometry and generic manifold fitting. Interna-\ntional Journal of Computer Vision , 50:35–61, 2002. 2\n[63] Duncan J Watts. Networks, dynamics, and the small-world\nphenomenon. American Journal of sociology , 105(2):493–\n527, 1999. 4\n[64] Xin Wu, Hao Zhao, Shunkai Li, Yingdian Cao, and Hongbin\nZha. Sc-wls: Towards interpretable feed-forward camera re-\nlocalization. In European Conference on Computer Vision ,\npages 585–601. Springer, 2022. 1\n[65] Heng Yang, Pasquale Antonante, Vasileios Tzoumas, and\nLuca Carlone. Graduated non-convexity for robust spatial\nperception: From non-minimal solvers to global outlier re-\njection. IEEE Robotics and Automation Letters , 5(2):1127–\n1134, 2020. 2\n[66] Heng Yang, Jingnan Shi, and Luca Carlone. Teaser: Fast']
 ['computer vision . Prentice Hall PTR, 2004. 2[42] Prajval Kumar Murali, Anirvan Dutta, Michael Gentner, Eti-\nenne Burdet, Ravinder Dahiya, and Mohsen Kaboli. Active\nvisuo-tactile interactive robotic perception for accurate ob-\nject pose estimation in dense clutter. IEEE Robotics and Au-\ntomation Letters , 7(2):4686–4693, 2022. 2\n[43] Carl Olsson, Fredrik Kahl, and Magnus Oskarsson. Branch-\nand-bound methods for euclidean registration problems.\nIEEE Transactions on Pattern Analysis and Machine Intel-\nligence , 31(5):783–794, 2008. 2\n[44] G Dias Pais, Srikumar Ramalingam, Venu Madhav Govindu,\nJacinto C Nascimento, Rama Chellappa, and Pedro Miraldo.\n3dregnet: A deep neural network for 3d point registration.\nInProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition , pages 7193–7203, 2020. 7\n[45] Victor Y Pan and Zhao Q Chen. The complexity of the matrix\neigenproblem. In Proceedings of the thirty-first annual ACM']
 ['tion search. In Computer Vision–ACCV 2012: 11th Asian\nConference on Computer Vision, Daejeon, Korea, November\n5-9, 2012, Revised Selected Papers, Part II 11 , pages 539–\n551. Springer, 2013. 2\n[5] Paul J Besl and Neil D McKay. Method for registration of\n3-d shapes. In Sensor fusion IV: control paradigms and data\nstructures , pages 586–606. Spie, 1992. 2\n[6] Thomas M Breuel. Implementation techniques for geometric\nbranch-and-bound matching methods. Computer Vision and\nImage Understanding , 90(3):258–294, 2003. 2\n[7] Alvaro Parra Bustos and Tat-Jun Chin. Guaranteed outlier\nremoval for point cloud registration with correspondences.\nIEEE transactions on pattern analysis and machine intelli-\ngence , 40(12):2868–2882, 2017. 2\n[8] C Cao, H Zhu, Z Ren, H Choset, and J Zhang. Represen-\ntation granularity enables time-efficient autonomous explo-\nration in large, complex worlds. Science Robotics , 8(80):\neadf0970, 2023. 1\n[9] Luca Carlone. Estimation contracts for outlier-robust geo-']]","
NA",What is the title of this paper?
2,2306.07745.pdf,"[['problem whether the suboptimal regret bounds in the case of standard optimistic LSVI policies (such\nas KOVI, Yang et al., 2020a) represent a fundamental shortcoming or an artifact of the proof.\nFunding Disclosure\nThis work was funded by MediaTek Research.\n10References\nAbbasi-Yadkori, Y . (2013). Online learning for linearly parametrized control problems. PhD Thesis,\nUniversity of Alberta .\nAbbasi-Yadkori, Y ., Pál, D., and Szepesvári, C. (2011). Improved algorithms for linear stochastic\nbandits. Advances in Neural Information Processing Systems , 24.\nAgrawal, S. and Goyal, N. (2013). Thompson sampling for contextual bandits with linear payoffs. In\nInternational conference on machine learning , pages 127–135. PMLR.\nArora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R. R., and Wang, R. (2019). On exact computation\nwith an infinitely wide neural net. Advances in neural information processing systems , 32.']
 ['f∈ BRcan be written as\nf=∞X\nm=1wm√σmϕm, (32)\nwithP∞\nm=1w2\nm≤R2. For some D∈N, letΠD[f]denote the projection of fonto the D-\ndimensional RKHS corresponding to the first Dfeatures with the largest eigenvalues\nΠD[f] =DX\nm=1wm√σmϕm. (33)\nLet us use the notations wD= [w1, w2,···, wD]⊤for the D-dimensional column vector of weights,\nϕD(z) = [ϕ1(z), ϕ2(z),···, ϕD(z)]⊤for the D-dimensional column vector of eigenfeatures, and\nΣD=diag([σ1, σ2,···, σD])for the diagonal matrix of eigenvalues with [σ1, σ2,···, σD]as the\ndiagonal entries. We also use the notations\nkD(z, z′) =ϕ⊤\nD(z)ΣDϕD(z), (34)\nto denote the kernel corresponding to the D-dimensional RKHS, as well as k0(z, z′) =k(z, z′)−\nkD(z, z′).\nC.1 Proof of Lemma 2 on Maximum Information Gain\nRecall the definition of Γk,λ(t). We have\n1\n2logdet\x12\nI+1\nλ2KZt\x13\n=1\n2logdet\x12\nI+1\nλ2(KD\nZt+K0\nZt)\x13\n=1\n2logdet\x12\nI+1\nλ2KD\nZt\x13\n| {z }\nTerm (i)+1\n2logdet\x12\nI+1\nλ2(I+1\nλ2KD\nZt)−1K0\nZt\x13\n| {z }\nTerm (ii).\n17We next bound the two terms on the right hand side.']
 ['optimization with adaptive sketching: scalable and no regret. In Proceedings of the Thirty-Second\nConference on Learning Theory , volume 99 of Proceedings of Machine Learning Research ,\nPhoenix, USA. PMLR.\nChowdhury, S. R. and Gopalan, A. (2017). On kernelized multi-armed bandits. In International\nConference on Machine Learning , pages 844–853. PMLR.\nChowdhury, S. R. and Gopalan, A. (2019). Online learning in kernelized Markov decision processes.\nInThe 22nd International Conference on Artificial Intelligence and Statistics , pages 3197–3205.\nPMLR.\nChristmann, A. and Steinwart, I. (2008). Support Vector Machines . Springer New York, NY .\nDomingues, O. D., Ménard, P., Pirotta, M., Kaufmann, E., and Valko, M. (2021). Kernel-based\nreinforcement learning: A finite-time analysis. In International Conference on Machine Learning ,\npages 2783–2792. PMLR.\nFawzi, A., Balog, M., Huang, A., Hubert, T., Romera-Paredes, B., Barekatain, M., Novikov, A.,']
 ['Phoenix, USA. PMLR.\nChowdhury, S. R. and Gopalan, A. (2017). On kernelized multi-armed bandits. In International\nConference on Machine Learning , pages 844–853. PMLR.\nChowdhury, S. R. and Gopalan, A. (2019). Online learning in kernelized Markov decision processes.\nInThe 22nd International Conference on Artificial Intelligence and Statistics , pages 3197–3205.\nPMLR.\nChristmann, A. and Steinwart, I. (2008). Support Vector Machines . Springer New York, NY .\nDomingues, O. D., Ménard, P., Pirotta, M., Kaufmann, E., and Valko, M. (2021). Kernel-based\nreinforcement learning: A finite-time analysis. In International Conference on Machine Learning ,\npages 2783–2792. PMLR.\nFawzi, A., Balog, M., Huang, A., Hubert, T., Romera-Paredes, B., Barekatain, M., Novikov, A.,\nR Ruiz, F. J., Schrittwieser, J., Swirszcz, G., et al. (2022). Discovering faster matrix multiplication\nalgorithms with reinforcement learning. Nature , 610(7930):47–53.']]","
Discovering faster matrix multiplication algorithms with reinforcement learning.",What is the title of this paper?
3,9110037.pdf,"[['[35] T.S. Bunch and P.C.W. Davies, Proc. R. Soc. A360 (1978) 117.\n[36] A. Vilenkin and L. Ford, Phys. Rev. D26(1982) 1231;\nA.D. Linde, Phys. Lett. 116B (1982) 335;\nA.A. Starobinsky, Phys. Lett. 117B (1982) 175.\n[37] A.D. Linde, Phys. Lett. 175B (1986) 395; Physica Scripta T15(1987)\n169; Physics Today 40(1987) 61.\n[38] A.D. Linde, Phys. Lett. B211 (1988) 29.\n[39] J.M. Cline, Nucl. Phys. B345 (1990) 281.\n[40] A. Mezhlumian, in preparation.\n27']
 ['M. Miji´ c, M. Sher and L. Susskind for valuable discussions a t diﬀerent stages\nof this investigation. This work was supported in part by NSF grant PHY-\n8612280.\n24References\n[1] A.D. Linde, Phys. Lett. B200 (1988) 272.\n[2] S. Coleman, Nucl. Phys. B307 (1988) 867.\n[3] T. Banks, Nucl. Phys. B309 (1988) 493.\n[4] S. Giddings and A. Strominger, Nucl. Phys. B307 (1988) 854.\n[5] S. Coleman, Nucl. Phys. B310 (1989) 643.\n[6] S. Giddings and A. Strominger, Nucl. Phys. B321 (1989) 481.\n[7] G.V. Lavrelashvili, V.A. Rubakov and P.G. Tinyakov, Mod . Phys. Lett.\nA3(1988) 1231;\nV.A. Rubakov and P.G. Tinyakov, Phys. Lett. 214B (1988) 334.\n[8] I. Klebanov, L. Susskind and T. Banks, Nucl. Phys. B317 (1989) 665.\n[9] A.D. Linde, Phys. Lett. B227 (1989) 352.\n[10] A.S. Goncharov, A.D. Linde and V.F. Mukhanov, Int. J. Mo d. Phys.\nA2(1987) 561.\n[11] S.W. Hawking, Phys. Lett. B195 (1987) 277; Phys. Rev. D37(1988)\n904.\n[12] G.V. Lavrelashvili, V.A. Rubakov and P.G. Tinyakov, JE TP Lett. 46']
 ['wood, New York, 1990).\n[15] J.B. Hartle and S.W. Hawking, Phys. Rev. D28(1983) 2960.\n[16] A.D. Linde, JETP 60(1984) 211; Lett. Nuovo Cim. 39(1984) 401;\nYa.B. Zeldovich and A.A. Starobinsky, Sov. Astron. Lett. 10(1984)\n135; V.A. Rubakov, Phys. Lett. 148B (1984) 280; A. Vilenkin, Phys.\nRev.D30(1984) 549.\n25[17] A.D. Linde, in: Proceedings of the 1989 Summer School in High\nEnergy Physics and Cosmology , eds. J. Pati, S. Randjbar-Daemi,\nE. Sezgin and Q. Shaﬁ (World Scientiﬁc, Singapore, 1989).\n[18] A.D. Linde, Physica Scripta T36(1991) 30.\n[19] E. Farhi and A. Guth, Phys. Lett. 183B (1987) 149.\n[20] W. Fischler, D. Morgan and J. Polchinski, Phys. Rev. D41(1990) 2638;\nPhys. Rev. D42(1990) 4042.\n[21] E. Farhi, A. Guth and J. Gueven, Nucl. Phys. B339 (1990) 417.\n[22] D. La and P.J. Steinhardt, Phys. Rev. Lett. 62(1989) 376.\n[23] A.D. Linde, Phys. Lett. B249 (1990) 18.\n[24] M.B. Voloshin, I.B. Kobzarev and L.B. Okun, Sov. J. Nucl . Phys. 20\n(1974) 644.']
 ['[37] A.D. Linde, Phys. Lett. 175B (1986) 395; Physica Scripta T15(1987)\n169; Physics Today 40(1987) 61.\n[38] A.D. Linde, Phys. Lett. B211 (1988) 29.\n[39] J.M. Cline, Nucl. Phys. B345 (1990) 281.\n[40] A. Mezhlumian, in preparation.\n27']]","
The title of this paper is ""Inflationary Cosmology in the Light of the Latest Observations"".",What is the title of this paper?
4,0911.0652.pdf,"[['experimentally validated ontology.  Further, refi ning our methods for generating PSSM libraries has \nthe potential to exponentially increas e the structural/functional annotati on of all classes of proteins \nacross taxa.  Such an advance would have broad im pacts on human health and disease, as well as \nbasic science endeavors.   13 \nAcknowledgments \nThis work was supported by the Searle Young In vestigators Award and star t-up money from PSU \n(RLP), NCSA grant TG-MCB070027N (RLP, DVR), The National Science Foundation 428-15 691M \n(RLP, DVR), and The National Inst itutes of Health R01 GM087410-01 (RLP, DVR). This project was \nalso funded by a Fellowship from the Eberly College of Sciences and the Huck Institutes of the Life \nSciences (DVR) and a grant with t he Pennsylvania Department of H ealth using Tobacco Settlement \nFunds (DVR). The Department of Health specific ally disclaims responsibility for any analyses,']
 ['experimentally validated ontology.  Further, refi ning our methods for generating PSSM libraries has \nthe potential to exponentially increas e the structural/functional annotati on of all classes of proteins \nacross taxa.  Such an advance would have broad im pacts on human health and disease, as well as \nbasic science endeavors.   13 \nAcknowledgments \nThis work was supported by the Searle Young In vestigators Award and star t-up money from PSU \n(RLP), NCSA grant TG-MCB070027N (RLP, DVR), The National Science Foundation 428-15 691M \n(RLP, DVR), and The National Inst itutes of Health R01 GM087410-01 (RLP, DVR). This project was \nalso funded by a Fellowship from the Eberly College of Sciences and the Huck Institutes of the Life \nSciences (DVR) and a grant with t he Pennsylvania Department of H ealth using Tobacco Settlement \nFunds (DVR). The Department of Health specific ally disclaims responsibility for any analyses,']
 ['. \n \nAbstract \nJust as physicists strive to develop a TOE (th eory of everything), which explains and \nunifies the physical laws of th e universe, the life-sci entist wishes to uncover the TOE as it \nrelates to cellular systems.  Th is can only be achieved with a quantitative platform that can \ncomprehensively deduce and relate protein stru cture, functional, a nd evolution of genomes \nand proteomes in a comparative f ashion.  Were this perfected, proper analyses would start to \nuncover the underlying physical laws governing th e emergent behavior of biological systems \nand the evolutionary pressures responsible for func tional innovation.  In the near term, such \nmethodology would allow the vast  quantities of uncharacterized (e.g. metagenomic samples) \nprimary amino acid sequences to be rapidly deco ded.  Analogous to natural products found in \nthe Amazon, genomes of living orga nisms contain large numbers of proteins that would prove']
 ['c{çá|vá TÜv{|äxá\naÉäxÅuxÜ ECCL\nBrainstorming through the Sequence Universe:\nTheories on the Protein Problem 1Brainstorming through the Sequence Universe: Theories on \nthe Protein Problem \n \nKyung Dae Ko1,2,*, Yoojin Hong1,3,*, Gaurav Bhardwaj1,2,, Teresa M. Killick1,2, Damian B. \nvan Rossum1,2,¥, Randen L. Patterson1,2,¥ \n \n(1) Center for Computatio nal Proteomics, The Pennsyl vania State University  \n(2) Department of Biology, The Pennsylvania State University \n(3) Department of Computer Science,  The Pennsylvania State University \n \n* These authors contributed equally to this work \n¥ Address correspondence to:  Randen L. Patterson, 230 Life Sc ience Bldg, University Park , PA 16802. Tel: 001-814-865-1668; \nFax: 001-814-863-1357; E-mail:  rlp25@psu.edu\n. \n Damian B. van Rossum, 518 Wartik Laboratory, Un iversity Park, PA 16802. Tel: 001-814-863-\n1007; Fax: 001-814-86 3-1357; E-mail:  dbv10@psu.edu\n. \n \nAbstract']]","
Answer: Brainstorming through the Sequence Universe: Theories on the Protein Problem",What is the title of this paper?
5,2403.09580.pdf,"[['J. Pearl. Causality: Models, Reasoning and Inference . Cambridge University Press, 2009.\nT.S. Richardson, R.J. Evans, J.M. Robins, and I. Shpitser. N ested Markov properties for acyclic\ndirected mixed graphs. In UAI’12: Proceedings of the Twenty-Eighth Conference on Unc ertainty in\nArtiﬁcial Intelligence , page 13. ACM, 2012.\nP. Sellinger. A survey of graphical languages for monoidal c ategories. In B. Coecke, editor, New\nStructures for Physics, Lecture Notes in Physics , pages 289–355. Springer, 2011.\nI. Shpitser. Complete identiﬁcation methods for the causal hierarchy. Journal of Machine Learning\nResearch , pages 1941–1979, 2008.\nY. Yin and J. Zhang. Markov categories, causal theories, and the do-calculus. page arXiv:2204.04821,\n2022.\n11']
 ['In this paper, we have shown that purely syntactic causal ide ntiﬁcation can be performed using rel-\natively simple steps. We observe that the simplicity of this approach largely arises from the process-\ncentric formulation of directed causal modelling and the fa ct that manipulations of this model this can\nbe expressed in terms of functions of the signature of the cat egory in which this model is represented.\nThese steps are unambiguous and therefore easily implement ed in software.\nAlthough our approach relies on chain factorization of the o bserved process, we note that this is more\nof a mathematical convenience than a restriction. An altern ative development of our approach can use\ncomb disintegration in place of the ﬁxing operator described here. This would lea d to diﬀerent forms\nof the resulting interventional signatures which are, none theless equivalent exterior processes. Finally,']
 ['application to other, more elaborate forms of causal identi ﬁcation such as conditional causal eﬀects,\nthose arising through edge interventions, and more general forms of causal identiﬁcation by combining\nmultiple causal models, would be valuable. It would be inter esting to see the extent to which the\nsignature-based approach also simpliﬁes the formulation o f existing algorithms for these problems.\n10References\nE. Bareinboim, J.D. Correa, D. Ibeling, and T. Icard. On Pearl’s Hierarchy and the Foundations of\nCausal Inference . ACM Books, 2020.\nD. Cakiqi and M.A. Little. Non-probabilistic Markov catego ries for causal modeling in machine learn-\ning. In ACT 2022: Applied Category Theory , 2022.\nK. Cho and B. Jacobs. Disintegration and bayesian inversion v ia string diagrams. Mathemati-\ncal Structures in Computer Science , 29(7):938–971, March 2019. ISSN 1469-8072. doi: 10.1017/\ns0960129518000488.\nB. Fong. Causal theories: A categorical perspective on Bayesi an networks, 2013.']
 ['T.S. Richardson, R.J. Evans, J.M. Robins, and I. Shpitser. N ested Markov properties for acyclic\ndirected mixed graphs. In UAI’12: Proceedings of the Twenty-Eighth Conference on Unc ertainty in\nArtiﬁcial Intelligence , page 13. ACM, 2012.\nP. Sellinger. A survey of graphical languages for monoidal c ategories. In B. Coecke, editor, New\nStructures for Physics, Lecture Notes in Physics , pages 289–355. Springer, 2011.\nI. Shpitser. Complete identiﬁcation methods for the causal hierarchy. Journal of Machine Learning\nResearch , pages 1941–1979, 2008.\nY. Yin and J. Zhang. Markov categories, causal theories, and the do-calculus. page arXiv:2204.04821,\n2022.\n11']]","
Causality: Models, Reasoning and Inference",What is the title of this paper?
6,2403.07769.pdf,"[['https://www.researchgate.net/publication/2844339  \nWooldridge, M. (2009). An Introduction to MultiAgent Systems (2nd ed.). John Wiley & Sons.  \nWu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann, S., Kambadur, P., Rosenberg, D., & Mann, G. (2023). \nBloombergGPT: A Large Language Model for Finance . \nYang, H., Liu, X. -Y., & Wang, C. D. (2023). FinGPT: Open -Source Financial Large Language Models . \nhttp://arxiv.org/abs/2306.06031']
 ['Fama, E. F. (1970). Efficient Capital Markets: A Review of Theory and Empirical Work. The Journal of Finance , 25(2), \n383. https://doi.org/10.2307/2325486  \nHuang, M. -H., & Rust, R. T. (2018). Artificial Intelligence in Service. Journal of Service Research , 21(2), 155 –172. \nhttps://doi.org/10.1177/1094670517752459  \nJ. Pruitt, & T. Adlin. (2006). The Persona Lifecycle . Elsevier. https://doi.org/10.1016/B978 -0-12-566251 -2.X5000 -X \nJung, C. G. (2014a). The Archetypes and the Collective Unconscious. In Collected Works of C.G. Jung, Volume 9 (Part \n1): Archetypes and the Collective Unconscious . Routledge. https://doi.org/10.4324/9781315725642  \nJung, C. G. (2014b). Two Essays on Analytical Psychology . Routledge. https://doi.org/10.4324/9781315725895  \nKimball, R., & Ross, M. (2015). The Kimball Group Reader . Wiley. https://doi.org/10.1002/9781119228912  \nLangton, N. , R. S. P. , & J. T. A. (2019). Organizational behaviour: Concepts, controversies, applications  (Pearson']
 ['Future research can focus on more advanced approaches, considering other industries, other activities, other \npersonas, for example, performing validations and more robust analysis of results, aiming to create more \nadaptable and effective systems to deal wi th organizational challenges in a constantly changing environment. \nevolution.  \nThe platform mentioned in this study is evolving and new developments are expected to become useful to \nhumans in their various areas of knowledge, enabling AI to transform competition into real collaboration, a \nwindow that opens to the resilient future wit h the use competent and creative Artificial Intelligence.  \n \n7 References  \n \nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, \nA., Agarwal, S., Herbert -Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter,']
 ['Norman/dp/0465067107/ref=wl_it_dp_o_pC_S_nC?ie=UTF8&colid=151193SNGKJT9&coliid=I262V9ZRW8HR2C  \nPorter, M. E. (1980). Competitive Strategy . Macat Library. https://doi.org/10.4324/9781912281060  \nRasal, S., & Hauer, E. J. (2024). NAVIGATING COMPLEXITY: ORCHESTRATED PROBLEM SOLVING WITH MULTI -AGENT \nLLMS . \nReed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth -Maron, G., Gimenez, M., Sulsky, Y., Kay, J., \nSpringenberg, J. T., Eccles, T., Bruce, J., Razavi, A., Edwards, A., Heess, N., Chen, Y., Hadsell, R., Vinyals, O., \nBordbar, M., & de Fr eitas, N. (2022). A Generalist Agent . http://arxiv.org/abs/2205.06175  \nRevella, A. (2015). Buyer Personas: How to Gain Insight into your Customer’s Expectations, Align your Marketing \nStrategies, and Win More Business . Wylei.  \nRussel, S., & Norvig Peter. (2022). Artificial Intelligence: Vol. Fourth Edition . \nSaunders, A. C. M. (2018). Financial   Institutions   Management A Risk Management Approach  (Ninth Edition).']]","
The Archetypes and the Collective Unconscious.",What is the title of this paper?
7,9502007.pdf,"[['puters, NPAC technical report SCCS-279 (1992).\n[39] A. Coniglio and W. Klein, J. Phys. A, 13, (1980) 2775. p\n[40] A. Coniglio, Phys. Rev. Lett. ,62, (1989) 3054.\n[41] J. Hoshen and R. Kopelman, Phys. Rev. B, 14, (1976) 3438.\n[42] U. Wolﬀ, Phys. Lett. B 228, (1989) 379 .\n[43] C. Munkel, Int. Jour. of Mod. Phys. C 4, 79 (1993).\n[44] H. J. Herrmann and H. E. Stanley, J. Phys. A 21L829 (1988).\n[45] E. N. Miranda, Physica A 175, 235 (1991), 179, 340 (1991); Physica A 175, 229 (1991).\n[46] D. Stauﬀer and J. Kertesz, Physica A 177, 381 (1991).\n[47] R. Hackl, Personal Communication.\n[48] L. Colonna-Romano, A. I. Mel´ cuk, H. Gould, and W. Klein ,Physica A 209, 396 (1994).\n[49] H. K. Janssen, B. Schaub and B. Schmittmann, Z. Phys. B 73, 539 (1988).\n11This figure ""fig1-1.png"" is available in ""png""\n format from:\nhttp://arxiv.org/ps/hep-lat/9502007v1This figure ""fig2-1.png"" is available in ""png""\n format from:\nhttp://arxiv.org/ps/hep-lat/9502007v1This figure ""fig2-2.png"" is available in ""png""']
 ['(1990); U. Wolﬀ, Phys. Rev. Lett. 60, 1461 (1988); R. Edwards and A. Sokal, Phys. Rev. D 38, 2009 (1988); W.\nKlein, T. Ray and P. Tamayo, Phys. Rev. Lett. ,62, 163 (1989); P. Tamayo, R. C. Brower and W. Klein, J. of Stat.\nPhys.,58, 1083 (1990).\n[23] T. H. Cormen, C. E. Leiserson and R. L. Rivest, Introduction to Algorithms , MIT Press, (1990); D. Knuth, The Art\nof Computer Programming, vol 3, Addison-Wesley (1973).\n[24] E. Horowitz and S. Sahni, Fundamentals of Computers Algorithms , Potomac, Md. Computer Science Press, 1978.\n[25] Y. Shiloach and U. Vishkin, Jour. of Algorithms 3, 57 (1982).\n10[26] U. Vishkin, Discrete Applied Mathematics 9, 197 (1984).\n[27] M. J. Quinn and N. Deo, Computing Surveys, Vol. 16, No 3, September 1984.\n[28] P. S. Gopalakrishnan, I. V. Ramakrishnan and L. N. Kanal , 1985IEEE, Int. Conf. on Parallel Processing.\n[29] W. Lim, A. Agrawal and L. Nekludova, Thinking Machines Tech. Report NA86-2.']
 ['[21] R. Swendsen and J. S. Wang, Phys. Rev. Lett. ,58,86 (1987).\n[22] R. Brower and P. Tamayo, Phys. Rev. Lett. ,62, 1087 (1989); R. Brower and S. Huang, Phys. Rev. D 41, 708\n(1990); U. Wolﬀ, Phys. Rev. Lett. 60, 1461 (1988); R. Edwards and A. Sokal, Phys. Rev. D 38, 2009 (1988); W.\nKlein, T. Ray and P. Tamayo, Phys. Rev. Lett. ,62, 163 (1989); P. Tamayo, R. C. Brower and W. Klein, J. of Stat.\nPhys.,58, 1083 (1990).\n[23] T. H. Cormen, C. E. Leiserson and R. L. Rivest, Introduction to Algorithms , MIT Press, (1990); D. Knuth, The Art\nof Computer Programming, vol 3, Addison-Wesley (1973).\n[24] E. Horowitz and S. Sahni, Fundamentals of Computers Algorithms , Potomac, Md. Computer Science Press, 1978.\n[25] Y. Shiloach and U. Vishkin, Jour. of Algorithms 3, 57 (1982).\n10[26] U. Vishkin, Discrete Applied Mathematics 9, 197 (1984).\n[27] M. J. Quinn and N. Deo, Computing Surveys, Vol. 16, No 3, September 1984.']
 ['Monte Carlo Study Phys. Rev. B 44, 5081 (1991).\n[7] C. F. Baillie, R. Gupta, K. Hawick and G. S. Pawley, Monte C arlo Renormalization-Group Study of the Three-\nDimensional Ising Model Phys. Rev. B 45, 10438 (1992); P. D. Coddington and C. F. Baillie, Phys. Rev. Lett. 68\n(1992) 962.\n[8] A. N. Burkitt and D. W. Heermann, Comp. Phys. Comm. 54, (1989) 210; D. W. Heermann and A. N. Burkitt,\nParallel Algorithms in Computational Science , Springer Verlag, Heidelberg 1991.\n[9] P. Tamayo, Physica A 201, 543 (1993).\n[10] C. F. Baillie and P. D. Coddington, Concurrency: Practice and Experience 3(2), 129 (1991); C. F. Baillie and P.D.\nCoddington, Phys. Rev. B 43, 10617 (1991);\n[11] R. C. Brower, P. Tamayo and B. York, Jour. of Stat. Phys. 63, (1991) 73.\n[12] J. Apostolakis, P. Coddington and E. Marinari, Europhys. Lett. 17(3), 198 (1992).\n[13] H. Mino, Comp. Phys. Comm. 66, 25 (1991).\n[14] D. Stauﬀer, Physica A 171, 471 (1991).']]","
NA",What is the title of this paper?
8,2403.07794.pdf,"[['the 2018 Conference on Empirical Methods in Natural\nLanguage Processing , 2018.\nConover, M., Hayes, M., Mathur, A., Xie, J., Wan, J., Shah,\nS., Ghodsi, A., Wendell, P., Zaharia, M., and Xin, R. Free\nDolly: Introducing the world’s first truly open instruction-\ntuned LLM. Online blog, 2023.\nDai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang,\nW., Li, B., Fung, P., and Hoi, S. InstructBLIP: Towards\ngeneral-purpose vision-language models with instruction\ntuning. arXiv preprint arXiv:2305.06500 , 2023.\nEtxaniz, J., Azkune, G., Soroa, A., de Lacalle, O. L., and\nArtetxe, M. Do multilingual language models think better\nin English? arXiv preprint arXiv:2308.01223 , 2023.\nGoyal, S., Ji, Z., Rawat, A. S., Menon, A. K., Kumar,\nS., and Nagarajan, V . Think before you speak: Train-\ning language models with pause tokens. arXiv preprint\narXiv:2310.02226 , 2023.\nGoyal, Y ., Khot, T., Summers-Stay, D., Batra, D., and\nParikh, D. Making the V in VQA matter: Elevating']
 ['Acknowledgements\nThis work has made use of the resources provided\nby the Edinburgh Compute and Data Facility (ECDF)\n(http://www.ecdf.ed.ac.uk/) and the Baskerville Tier 2 HPC\nservice (https://www.baskerville.ac.uk/). Baskerville was\nfunded by the EPSRC and UKRI through the World Class\nLabs scheme (EP/T022221/1) and the Digital Research In-\nfrastructure programme (EP/W032244/1) and is operated by\nAdvanced Research Computing at the University of Birm-\ningham.\nReferences\nAnsell, A., Parovi ´c, M., Vuli ´c, I., Korhonen, A., and Ponti,\nE. Unifying cross-lingual transfer across scenarios of\nresource scarcity. In Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Processing ,\n2023.\nArtetxe, M., Ruder, S., and Yogatama, D. On the cross-\nlingual transferability of monolingual representations. In\nProceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics , 2020.\nArtetxe, M., Goswami, V ., Bhosale, S., Fan, A., and Zettle-']
 ['Conference on Human Factors in Computing Systems ,\n2022.\nZhang, T., Kishore, V ., Wu, F., Weinberger, K. Q., and Artzi,\nY . BERTScore: Evaluating text generation with BERT. In\nInternational Conference on Learning Representations ,\n2020.\nZhang, Z., Lee, D.-H., Fang, Y ., Yu, W., Jia, M., Jiang,\nM., and Barbieri, F. PLUG: Leveraging pivot lan-\nguage in cross-lingual instruction tuning. arXiv preprint\narXiv:2311.08711 , 2023.\nZhou, D., Sch ¨arli, N., Hou, L., Wei, J., Scales, N., Wang,\nX., Schuurmans, D., Cui, C., Bousquet, O., Le, Q. V .,\nand Chi, E. H. Least-to-most prompting enables complex\nreasoning in large language models. In The Eleventh\nInternational Conference on Learning Representations ,\n2023.\n11']
 ['Parikh, D. Making the V in VQA matter: Elevating\nthe role of image understanding in visual question an-\nswering. In Conference on Computer Vision and Pattern\nRecognition , 2017.\nHu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,\nS., Wang, L., and Chen, W. LoRA: Low-rank adaptation\nof large language models. In International Conference\non Learning Representations , 2022.\nHuang, H., Tang, T., Zhang, D., Zhao, X., Song, T., Xia,\nY ., and Wei, F. Not all languages are created equal\nin LLMs: Improving multilingual capability by cross-\nlingual-thought prompting. In Findings of the Association\nfor Computational Linguistics: EMNLP 2023 , 2023.\nHudson, D. A. and Manning, C. D. GQA: A new dataset for\nreal-world visual reasoning and compositional question\nanswering. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , 2019.\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,']]","
The 2018 Conference on Empirical Methods in Natural Language Processing",What is the title of this paper?
9,2311.14682.pdf,"[['written on this topic, some of these were compiled in the 2021 paper [12] and the 2022 paper [9].\nThese models attempt at studying on a global or local scale, different parameters, related to the\npandemics, such as mortality, etc. Regardless of their objectives, those models used different types\n∗American University of Beirut (AUB), Beirut, Lebanon. (rhm26@mail.aub.edu)\n†American University of Beirut (AUB), Beirut, Lebanon. (sm101@aub.edu.lb)\n‡American University of Beirut (AUB), Beirut, Lebanon. (nn12@aub.edu.lb)\n§The authors would like to acknowledge:\n•The programming contribution of Miss Nadine Charaf and Miss Lara Tarakh during the Summer Research Camp 2022\n•AUB Center for Advanced Mathematical Sciences (CAMS) and Math department’s support\n1arXiv:2311.14682v1  [q-bio.PE]  2 Nov 2023of approaches: some are strictly relying on statistical software, others use innovative data science\ncombined to artificial intelligence. Other approaches were simply of compartmental nature.']
 ['written on this topic, some of these were compiled in the 2021 paper [12] and the 2022 paper [9].\nThese models attempt at studying on a global or local scale, different parameters, related to the\npandemics, such as mortality, etc. Regardless of their objectives, those models used different types\n∗American University of Beirut (AUB), Beirut, Lebanon. (rhm26@mail.aub.edu)\n†American University of Beirut (AUB), Beirut, Lebanon. (sm101@aub.edu.lb)\n‡American University of Beirut (AUB), Beirut, Lebanon. (nn12@aub.edu.lb)\n§The authors would like to acknowledge:\n•The programming contribution of Miss Nadine Charaf and Miss Lara Tarakh during the Summer Research Camp 2022\n•AUB Center for Advanced Mathematical Sciences (CAMS) and Math department’s support\n1arXiv:2311.14682v1  [q-bio.PE]  2 Nov 2023of approaches: some are strictly relying on statistical software, others use innovative data science\ncombined to artificial intelligence. Other approaches were simply of compartmental nature.']
 ['Data-Driven Models for studying the Dynamics of the\nCOVID-19 Pandemics\nRawan Madi∗Sophie Moufawad†Nabil Nassif‡§\nNovember 28, 2023\nAbstract\nThis paper seeks to study the evolution of the COVID-19 pandemic based on daily published\ndata from Worldometer website, using a time-dependent SIR model. Our findings indicate that\nthis model fits well such data, for different chosen periods and different regions.\nThis well-known model, consisting of three disjoint compartments, susceptible , infected , and\nremoved , depends in our case on two time dependent parameters, the infection rate β(t) and the\nremoval rate ρ(t). After deriving the model, we prove the local exponential behavior of the number\nof infected people, be it growth or decay. Furthermore, we extract a time dependent replacement\nfactor σs(t) =β(t)s(t)/ρ(t), where s(t) is the ratio of susceptible people at time t. In addition,\ni(t) and r(t) are respectively the ratios of infected and removed people, based on a population of']
 ['Data-Driven Models for studying the Dynamics of the\nCOVID-19 Pandemics\nRawan Madi∗Sophie Moufawad†Nabil Nassif‡§\nNovember 28, 2023\nAbstract\nThis paper seeks to study the evolution of the COVID-19 pandemic based on daily published\ndata from Worldometer website, using a time-dependent SIR model. Our findings indicate that\nthis model fits well such data, for different chosen periods and different regions.\nThis well-known model, consisting of three disjoint compartments, susceptible , infected , and\nremoved , depends in our case on two time dependent parameters, the infection rate β(t) and the\nremoval rate ρ(t). After deriving the model, we prove the local exponential behavior of the number\nof infected people, be it growth or decay. Furthermore, we extract a time dependent replacement\nfactor σs(t) =β(t)s(t)/ρ(t), where s(t) is the ratio of susceptible people at time t. In addition,\ni(t) and r(t) are respectively the ratios of infected and removed people, based on a population of']]","
Data-Driven Models for studying the Dynamics of the COVID-19 Pandemics",What is the title of this paper?
10,2402.06779.pdf,"[['structure on short-term evolutionary rescue in the face of a novel pathogenic\nthreat. Glob. Ecol. Conserv. 23(e01174), 01174 (2020)\n27']
 ['Source-sink dynamics in a two-patch SI epidemic\nmodel with life stages and no recovery from\ninfection\nJimmy Calvo-Monge1, Jorge Arroyo-Esquivel2*, Alyssa Gehman3,\nFabio Sanchez1,4\n1Escuela de Matem´ atica, Universidad de Costa Rica, San Pedro, 11501,\nSan Jos´ e, Costa Rica.\n2*Department of Global Ecology, Carnegie Institution for Science,\nWashington DC, 20015, United States of America.\n3Hakai Institute, 25039, British Columbia, Canada.\n4Centro de Investigaci´ on en Matem´ atica Pura y Aplicada, Universidad\nde Costa Rica, San Pedro, 11501, San Jos´ e, Costa Rica.\n*Corresponding author(s). E-mail(s):\njarroyoesquivel@carnegiescience.edu;\nContributing authors: jimmy.calvo@ucr.ac.cr; alyssa.gehman@hakai.org;\nfabio.sanchez@ucr.ac.cr;\nAbstract\nThis study presents a comprehensive analysis of a two-patch, two-life stage SI\nmodel without recovery from infection, focusing on the dynamics of disease\nspread and host population viability in natural populations. The model, inspired']
 ['Source-sink dynamics in a two-patch SI epidemic\nmodel with life stages and no recovery from\ninfection\nJimmy Calvo-Monge1, Jorge Arroyo-Esquivel2*, Alyssa Gehman3,\nFabio Sanchez1,4\n1Escuela de Matem´ atica, Universidad de Costa Rica, San Pedro, 11501,\nSan Jos´ e, Costa Rica.\n2*Department of Global Ecology, Carnegie Institution for Science,\nWashington DC, 20015, United States of America.\n3Hakai Institute, 25039, British Columbia, Canada.\n4Centro de Investigaci´ on en Matem´ atica Pura y Aplicada, Universidad\nde Costa Rica, San Pedro, 11501, San Jos´ e, Costa Rica.\n*Corresponding author(s). E-mail(s):\njarroyoesquivel@carnegiescience.edu;\nContributing authors: jimmy.calvo@ucr.ac.cr; alyssa.gehman@hakai.org;\nfabio.sanchez@ucr.ac.cr;\nAbstract\nThis study presents a comprehensive analysis of a two-patch, two-life stage SI\nmodel without recovery from infection, focusing on the dynamics of disease\nspread and host population viability in natural populations. The model, inspired']
 ['(Pycnopodia helianthoides) (2023)\n[25] Kang, Y., Castillo-Chavez, C., Levin, S.A.: A simple two-patch epidemiolog-\nical model with allee effects and disease-modified fitness. (2012). https://api.\nsemanticscholar.org/CorpusID:14056963\n[26] Tewa, J.J., Bowong, S., Mewoli, B.: Mathematical analysis of two-patch model\nfor the dynamical transmission of tuberculosis. Applied Mathematical Modelling\n36(6), 2466–2485 (2012) https://doi.org/10.1016/j.apm.2011.09.004\n[27] Arino, J., Sun, C., Yang, W.: Revisiting a two-patch SIS model with infection\nduring transport. Math. Med. Biol. 33(1), 29–55 (2016) https://doi.org/10.1093/\nimammb/dqv001\n[28] Calvo, J.G., Hern´ andez, A., Porter, M.A., Sanchez, F.: A two-patch epidemic\nmodel with nonlinear reinfection. Rev. Mat. [online] 27(1), 23–48 (2020) https:\n//doi.org/10.15517/rmta.v27i1.39946\n[29] Lee, S., Baek, O., Melara, L.: Resource allocation in two-patch epidemic model\nwith state-dependent dispersal behaviors using optimal control. Processes 8(9)']]","
Source-sink dynamics in a two-patch SI epidemic model with life stages and no recovery from infection",What is the title of this paper?
11,2305.00002.pdf,"[['Proceedings of the IEEE conference on computer visio n and pattern recognition , pp. 1251 –\n1258.  \n13. CIE, C. (1932) ‘Commission internationale de l’eclairage proceedings, 1931’, Cambridge \nUniversity, Cambridge  [Preprint].  \n14. Czech, D., Mishra, A. and Inggs, M. (2018) ‘A CNN and LSTM -based approach to \nclassifying tran sient radio frequency interference’, Astronomy and computing , 25, pp. 52 –\n57. 42 \n 15. Dieleman, S., Willett, K.W. and Dambre, J. (2015) ‘Rotation -invariant convolutional neural \nnetworks for galaxy morphology prediction’, Monthly Notices of the Royal Astronomical \nSociety , 450(2), pp. 1441 –1459. doi:10.1093/mnras/stv632.  \n16. Dietterich, T.G. (2000) ‘Ensemble methods in machine learning’, in International workshop \non multiple classifier systems . Springer, pp. 1 –15. \n17. equasys GmbH (no date) Color Conversion . Available at: \nhttps://web.archive.org/web/20180423091842/http://www.equasys.de/colorconversion.ht\nml (Accessed: 27 March 2022).']
 ['CHAPTER 1  1 \n1.1 Background  1 \n1.2 Problem Statement and Related Research  2 \n1.3 Research Questions  3 \n1.4 Aim and Objectives  4 \n1.5 Significance of the Study  4 \n1.6 Scope of the Study  5 \nCHAPTER 2  6 \n2.1. Machine Learning in Astronomy  6 \n2.2. Convolutional Neural Netwo rks 7 \n2.2.1 AlexNet  7 \n2.2.2 VGGNet  8 \n2.2.3 GoogLeNet  9 \n2.2.4 Xception  9 \n2.2.5 ResNet  9 \n2.2.6 DenseNet  10 \n2.3. Pre -trained Model and Transfer Learning  10 \n2.3.1 Transfer Learning in Astronomy  10 \n2.3.2 Pre -trained Model as a Feature Extractor  11 \n2.4. Colour Space  12 \n2.4.1 Transformation to XYZ  12 \n2.4.2 Transformation to HSL  13 \n2.4.3 Transformation to HSV  13 \n2.4.4 Transformation to CIELAB  13 \n2.4.5 Transformation to CIELUV  14 \n2.4.6 Transformation to YUV  14 \n2.4.7 Transformation to YCbCr  15 \n2.4.8 Transformation to YDbDr  15 vi \n 2.4.9 Transformation to YIQ  15 \n2.4.10 Transformation to HED  15 \n2.5. Color Space Transformation in CNN  15 \n2.6. Deep Learning Ensemble  17']
 ['Analytical Methods’, in Seitter, W.C., Duerbeck, H.W., and Tacke, M. (eds). Be rlin, \nHeidelberg: Springer Berlin Heidelberg, pp. 315 –322. \n3. Atha, D.J. and Jahanshahi, M.R. (2017) ‘Evaluation of deep learning approaches based on \nconvolutional neural networks for corrosion detection’, Structural Health Monitoring , \n17(5), pp. 1110 –1128. d oi:10.1177/1475921717737051.  \n4. Awang Iskandar, D.N.F. et al.  (2020) ‘Classification of Planetary Nebulae through Deep \nTransfer Learning’, Galaxies . doi:10.3390/galaxies8040088.  \n5. Boser, B.E., Guyon, I.M. and Vapnik, V.N. (1992) ‘A Training Algorithm for Optim al \nMargin Classifiers’, in Proceedings of the Fifth Annual Workshop on Computational \nLearning Theory . New York, NY, USA: Association for Computing Machinery (COLT \n’92), pp. 144 –152. doi:10.1145/130385.130401.  \n6. Bozinovski, S. (2020) ‘Reminder of the first pa per on transfer learning in neural networks, \n1976’, Informatica , 44(3).']
 ['1.3 Research Questions  3 \n1.4 Aim and Objectives  4 \n1.5 Significance of the Study  4 \n1.6 Scope of the Study  5 \nCHAPTER 2  6 \n2.1. Machine Learning in Astronomy  6 \n2.2. Convolutional Neural Netwo rks 7 \n2.2.1 AlexNet  7 \n2.2.2 VGGNet  8 \n2.2.3 GoogLeNet  9 \n2.2.4 Xception  9 \n2.2.5 ResNet  9 \n2.2.6 DenseNet  10 \n2.3. Pre -trained Model and Transfer Learning  10 \n2.3.1 Transfer Learning in Astronomy  10 \n2.3.2 Pre -trained Model as a Feature Extractor  11 \n2.4. Colour Space  12 \n2.4.1 Transformation to XYZ  12 \n2.4.2 Transformation to HSL  13 \n2.4.3 Transformation to HSV  13 \n2.4.4 Transformation to CIELAB  13 \n2.4.5 Transformation to CIELUV  14 \n2.4.6 Transformation to YUV  14 \n2.4.7 Transformation to YCbCr  15 \n2.4.8 Transformation to YDbDr  15 vi \n 2.4.9 Transformation to YIQ  15 \n2.4.10 Transformation to HED  15 \n2.5. Color Space Transformation in CNN  15 \n2.6. Deep Learning Ensemble  17 \n2.6.1 Unweighted model averaging  17 \n2.6.2 Majority Voting  17 \nCHAPTER 3  18']]","
Answer: NA",What is the title of this paper?
12,1706.03762-6.pdf,"[[',\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw']
 ['model. In Empirical Methods in Natural Language Processing , 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859 , 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts']
 ['[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144 , 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers) , pages 434–443. ACL, August 2013.\n12Attention Visualizations\nInput-Input Layer5\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments']
 ['have\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\n13Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust']]","
The title of this paper is ""Attention is All you Need"".",What is the title of this paper?
13,2401.04398.pdf,"[['Technologies, Volume 1 (Long and Short Papers) , pp. 4171–4186, 2019. 3\nBhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-Wei Chang, Dipanjan Das, and William Co-\nhen. Handling divergent reference texts when evaluating table-to-text generation. In Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics , pp. 4884–4895,\n2019. 14\nJulian Eisenschlos, Syrine Krichene, and Thomas Müller. Understanding tables with intermediate\npre-training. In Findings of the Association for Computational Linguistics: EMNLP 2020 , pp.\n281–296, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/\n2020.findings-emnlp.27. 1, 3\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. PAL: Program-aided language models. In International Conference on Machine\nLearning , pp. 10764–10799. PMLR, 2023. 3, 6\nZihui Gu, Ju Fan, Nan Tang, Preslav Nakov, Xiaoman Zhao, and Xiaoyong Du. PASTA: Table-']
 ['Annual Meeting of the Association for Computational Linguistics , pp. 4320–4333, Online, July\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.398. 1, 3\nCheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner,\nRanjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger\nlanguage models with less training data and smaller model sizes. In Findings of the Association\nfor Computational Linguistics: ACL 2023 . Association for Computational Linguistics, 2023. 2\nShima Imani, Liang Du, and Harsh Shrivastava. MathPrompter: Mathematical reasoning using large\nlanguage models. In Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 5: Industry Track) , pp. 37–42, Toronto, Canada, July 2023. Association for\nComputational Linguistics. doi: 10.18653/v1/2023.acl-industry.4. 13']
 ['and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long\nPapers) , pp. 1470–1480, Beijing, China, July 2015. Association for Computational Linguistics.\ndoi: 10.3115/v1/P15-1142. 2, 5\nRichard Pönighaus. ’favourite’sql-statements—an empirical analysis of sql-usage in commercial\napplications. In International Conference on Information Systems and Management of Data , pp.\n75–91. Springer, 1995. 2\nNitarshan Rajkumar, Raymond Li, and Dzmitry Bahdanau. Evaluating the text-to-sql capabilities of\nlarge language models. arXiv preprint arXiv:2204.00498 , 2022. 3, 6\nTianze Shi, Chen Zhao, Jordan Boyd-Graber, Hal Daumé III, and Lillian Lee. On the potential\nof lexico-logical alignments for semantic parsing to sql queries. Findings of the Association for\nComputational Linguistics: EMNLP 2020 , 2020. 2\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-']
 ['bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) , pp. 4171–4186, 2019. 3\nBhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-Wei Chang, Dipanjan Das, and William Co-\nhen. Handling divergent reference texts when evaluating table-to-text generation. In Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics , pp. 4884–4895,\n2019. 14\nJulian Eisenschlos, Syrine Krichene, and Thomas Müller. Understanding tables with intermediate\npre-training. In Findings of the Association for Computational Linguistics: EMNLP 2020 , pp.\n281–296, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/\n2020.findings-emnlp.27. 1, 3\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and']]","
Technologies, Volume 1 (Long and Short Papers)",What is the title of this paper?
14,9301115.pdf,"[['[9] Rohit J. Parikh, “On context-free languages,” Journal of the ACM 13(1966), 570–581.\n10']
 ['arXiv:cs/9301115v1  [cs.DS]  1 Dec 1991Context-Free Multilanguages\nDonald E. Knuth\nComputer Science Department, Stanford University\nInspired by ideas of Chomsky, Bar-Hillel, Ginsburg, and the ir coworkers, I spent the summer of\n1964 drafting Chapter 11 of a book I had been asked to write. Th e main purpose of that book,\ntentatively entitled The Art of Computer Programming , was to explain how to write compilers;\ncompilation was to bethe subject of the twelfth and ﬁnal chap ter. Chapter 10 was called “Parsing,”\nand Chapter 11 was “The theory of languages.” I wrote the draf ts of these chapters in the order\n11, 10, 12, because Chapter 11 was the most fun to do.\nTerminology and notation for formal linguistics were in a gr eat state of ﬂux in the early 60s,\nso it was natural for me to experiment with new ways to deﬁne th e notion of what was then being\ncalled a “Chomsky type 2” or “ALGOL-like” or “deﬁnable” or “p hrase structure” or “context-free”']
 ['arXiv:cs/9301115v1  [cs.DS]  1 Dec 1991Context-Free Multilanguages\nDonald E. Knuth\nComputer Science Department, Stanford University\nInspired by ideas of Chomsky, Bar-Hillel, Ginsburg, and the ir coworkers, I spent the summer of\n1964 drafting Chapter 11 of a book I had been asked to write. Th e main purpose of that book,\ntentatively entitled The Art of Computer Programming , was to explain how to write compilers;\ncompilation was to bethe subject of the twelfth and ﬁnal chap ter. Chapter 10 was called “Parsing,”\nand Chapter 11 was “The theory of languages.” I wrote the draf ts of these chapters in the order\n11, 10, 12, because Chapter 11 was the most fun to do.\nTerminology and notation for formal linguistics were in a gr eat state of ﬂux in the early 60s,\nso it was natural for me to experiment with new ways to deﬁne th e notion of what was then being\ncalled a “Chomsky type 2” or “ALGOL-like” or “deﬁnable” or “p hrase structure” or “context-free”']
 ['(Reading, Mass.: Addison-Wesley, 1969).\n[8] Donald E. Knuth, “Top-down syntax analysis,” Acta Informatica 1(1971), 79–110.\n[9] Rohit J. Parikh, “On context-free languages,” Journal of the ACM 13(1966), 570–581.\n10']]","
Answer: Context-Free Multilanguages",What is the title of this paper?
15,2402.18651.pdf,"[['20(1):979–1006, 2019.\nCimini, G., Squartini, T., Saracco, F., Garlaschelli, D.,\nGabrielli, A., and Caldarelli, G. The statistical physics\nof real-world networks. Nature Reviews Physics , 1(1):58,\n2019.\nCrowston, K. Amazon mechanical turk: A research tool for\norganizations and information systems scholars. In Shap-\ning the future of ICT research: Methods and approaches ,\npp. 210–221. Springer, 2012.\nDeutsch, D. The beginning of infinity: Explanations that\ntransform the world . Penguin Books UK, 2011.\nDevaine, M., Hollard, G., and Daunizeau, J. The social\nbayesian brain: Does mentalizing make a difference\nwhen we learn? PLoS Computational Biology , 10(12):\ne1003992, 2014.\nDoya, K., Ishii, S., Pouget, A., and Rao, R. P. Bayesian\nbrain: Probabilistic approaches to neural coding . MIT\npress, 2007.Eichenbaum, H. The role of the hippocampus in navigation\nis memory. Journal of Neurophysiology , 117(4):1785–\n1796, 2017.\nField, D. J. What the statistics of natural images tell us']
 ['20(1):979–1006, 2019.\nCimini, G., Squartini, T., Saracco, F., Garlaschelli, D.,\nGabrielli, A., and Caldarelli, G. The statistical physics\nof real-world networks. Nature Reviews Physics , 1(1):58,\n2019.\nCrowston, K. Amazon mechanical turk: A research tool for\norganizations and information systems scholars. In Shap-\ning the future of ICT research: Methods and approaches ,\npp. 210–221. Springer, 2012.\nDeutsch, D. The beginning of infinity: Explanations that\ntransform the world . Penguin Books UK, 2011.\nDevaine, M., Hollard, G., and Daunizeau, J. The social\nbayesian brain: Does mentalizing make a difference\nwhen we learn? PLoS Computational Biology , 10(12):\ne1003992, 2014.\nDoya, K., Ishii, S., Pouget, A., and Rao, R. P. Bayesian\nbrain: Probabilistic approaches to neural coding . MIT\npress, 2007.Eichenbaum, H. The role of the hippocampus in navigation\nis memory. Journal of Neurophysiology , 117(4):1785–\n1796, 2017.\nField, D. J. What the statistics of natural images tell us']
 ['such as this could offer insight into the effects of community\nsize and social norms on our priors over social networks.\nGeneral message. This paper offers a case-study about\nthe power of carefully constructed experiments and clever\nanalysis. Just as neural networks benefit from having archi-\n8Quantifying Human Priors over Social and Navigation Networks']
 ['of Experimental Psychology , 138:487–502, 2009.\nBravo-Hermsdorff, G. Quantifying Human Priors over\nAbstract Relational Structures . PhD thesis, Princeton\nUniversity, 2020.\nBravo-Hermsdorff, G., Gunderson, L. M., Maugis, P.-A.,\nand Priebe, C. E. Quantifying network similarity using\ngraph cumulants. arXiv:2107.11403 , 2021.\nCamargo Souza, L. Switch-reference as anaphora: A modu-\nlar account . PhD thesis, Rutgers University, 2020.\nCanini, K. R., Griffiths, T. L., Vanpaemel, W., and Kalish,\nM. L. Revealing human inductive biases for category\nlearning by simulating cultural transmission. Psycho-\nnomic Bulletin & Review , 21(3):785–793, 2014.\nChatterjee, S. and Diaconis, P. Estimating and understand-\ning exponential random graph models. The Annals of\nStatistics , 41(5):2428–2461, Oct 2013.\nChazelle, B. and Wang, C. Self-sustaining iterated learning.\narXiv:1609.03960 , 2016.\nChazelle, B. and Wang, C. Iterated learning in dynamic\nsocial networks. Journal of Machine Learning Research ,']]",Quantifying Human Priors over Social and Navigation Networks,What is the title of this paper?
16,2403.06970.pdf,"[['Seker. 2019. What’s wrong with Hebrew NLP?\nand how to make it right. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP): System Demonstrations , pages\n259–264, Hong Kong, China. Association for Com-\nputational Linguistics.\nShaked Yehezkel and Yuval Pinter. 2023. Incorporating\ncontext into subword vocabularies. In Proceedings\nof the 17th Conference of the European Chapter of\nthe Association for Computational Linguistics , pages\n623–635, Dubrovnik, Croatia. Association for Com-\nputational Linguistics.A Appendix: Hyperparameters\nThe expert classifiers of our model are all linear\nclassifiers, and thus their size is predetermined,\nsince they simply map from the hidden dimension\nsize of the base BERT model to a set number of\nlabels.\nFor the Dependency Tree Parsing expert, we\nhad to choose the desired attention-head dimension.']
 ['ysis. In Proceedings of the 23rd International Con-\nference on Computational Linguistics (Coling 2010) ,\npages 394–402, Beijing, China. Coling 2010 Orga-\nnizing Committee.\nEylon Gueta, Avi Shmidman, Shaltiel Shmidman,\nCheyn Shmuel Shmidman, Joshua Guedalia, Moshe\nKoppel, Dan Bareket, Amit Seker, and Reut Tsar-\nfaty. 2023. Large pre-trained models with extra-large\nvocabularies: A contrastive analysis of hebrew bert\nmodels and a new one to outperform them all.\nEliyahu Kiperwasser and Yoav Goldberg. 2016. Simple\nand accurate dependency parsing using bidirectional\nLSTM feature representations. Transactions of the\nAssociation for Computational Linguistics , 4:313–\n327.\nStav Klein and Reut Tsarfaty. 2020. Getting the ##life\nout of living: How adequate are word-pieces for mod-\nelling complex morphology? In Proceedings of the\n17th SIGMORPHON Workshop on Computational\nResearch in Phonetics, Phonology, and Morphology ,\npages 204–209, Online. Association for Computa-\ntional Linguistics.']
 ['given the context. In such cases, our parser is liable\nto make decisions based upon stereotypical gender\nroles, to the extent that these roles are reflected by\nthe texts in the corpus.\nAcknowledgements\nThe work of the second author has been supported\nby ISF grant 2617/22. The work of the last author\nhas been funded by the Israeli Ministry of Science\nand Technology (MOST) grant No. 3-17992, and\nby an Israeli Innovation Authority (IIA) KAMIN\ngrant, for which we are grateful.\nReferences\nDan Bareket and Reut Tsarfaty. 2021. Neural model-\ning for named entities and morphology (NEMO2).\nTransactions of the Association for Computational\nLinguistics , 9:909–928.\nShay B. Cohen and Noah A. Smith. 2007. Joint morpho-\nlogical and syntactic disambiguation. In Proceedings\nof the 2007 Joint Conference on Empirical Methods\nin Natural Language Processing and Computational\nNatural Language Learning (EMNLP-CoNLL) , pages\n208–217, Prague, Czech Republic. Association for\nComputational Linguistics.']
 ['the two words will be written with the same se-\nquence of letters (the difference between the words\nis indicated via the diacritics; yet the diacritics are\ngenerally omitted in written Hebrew texts). Thus,\nas part of the morphological tagging task, Hebrew\nparsers must predict whether these ambiguous writ-\nten words are in fact masculine or feminine forms,\ngiven the context. In such cases, our parser is liable\nto make decisions based upon stereotypical gender\nroles, to the extent that these roles are reflected by\nthe texts in the corpus.\nAcknowledgements\nThe work of the second author has been supported\nby ISF grant 2617/22. The work of the last author\nhas been funded by the Israeli Ministry of Science\nand Technology (MOST) grant No. 3-17992, and\nby an Israeli Innovation Authority (IIA) KAMIN\ngrant, for which we are grateful.\nReferences\nDan Bareket and Reut Tsarfaty. 2021. Neural model-\ning for named entities and morphology (NEMO2).\nTransactions of the Association for Computational']]","
Incorporating context into subword vocabularies.",What is the title of this paper?
